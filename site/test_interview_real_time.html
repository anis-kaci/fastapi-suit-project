<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interview Video Analyzer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Custom spinner animation */
        .spinner {
            border: 4px solid rgba(0, 0, 0, 0.1);
            border-left-color: #6366f1; /* Tailwind indigo-500 */
            border-radius: 50%;
            width: 32px;
            height: 32px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        /* Container for video and canvas overlay */
        .video-container {
            position: relative;
            width: 100%;
            max-width: 800px; /* Adjust as needed */
            margin: 0 auto;
            border-radius: 0.5rem; /* rounded-lg */
            overflow: hidden; /* Ensures contents stay within rounded borders */
            background-color: black; /* To show black background if video is not loaded */
        }
        .video-container video,
        .video-container canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: block;
        }
        .video-container video {
            z-index: 1; /* Video behind canvas */
            transform: scaleX(-1); /* Mirror webcam feed */
        }
        .video-container canvas {
            z-index: 2; /* Canvas on top of video */
        }
        /* Maintain aspect ratio with padding-bottom trick */
        .video-aspect-ratio {
            padding-bottom: 56.25%; /* 16:9 aspect ratio (height / width * 100) */
            position: relative;
            width: 100%;
        }
    </style>
</head>
<body class="bg-gray-100 flex items-center justify-center min-h-screen p-4">
    <div class="bg-white p-8 rounded-lg shadow-lg w-full max-w-4xl">
        <h1 class="text-3xl font-bold text-gray-800 mb-6 text-center">Analyseur de Vidéo d'Entretien IA</h1>
        <p class="text-gray-600 mb-6 text-center">Téléchargez une vidéo d'entretien ou utilisez votre webcam pour obtenir une analyse en temps réel des émotions faciales. Obtenez également la transcription audio et l'analyse du sentiment du texte pour les vidéos enregistrées.</p>

        <div class="flex border-b border-gray-200 mb-6">
            <button id="uploadTab" class="py-2 px-4 text-sm font-medium text-indigo-600 border-b-2 border-indigo-600 focus:outline-none">
                Analyser une vidéo enregistrée
            </button>
            <button id="webcamTab" class="py-2 px-4 text-sm font-medium text-gray-500 hover:text-gray-700 focus:outline-none">
                Analyser via Webcam (temps réel)
            </button>
        </div>

        <div id="uploadSection">
            <form id="uploadForm" class="space-y-4 mb-8">
                <label for="videoFile" class="block text-gray-700 font-medium text-lg">Sélectionnez une vidéo :</label>
                <input type="file" id="videoFile" name="video" accept="video/*" class="w-full p-3 border border-gray-300 rounded-md focus:ring-indigo-500 focus:border-indigo-500" required>
                
                <button type="submit" id="analyzeButton" class="w-full bg-indigo-600 text-white p-3 rounded-md font-semibold hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 transition-colors">
                    Analyser la Vidéo
                </button>
            </form>
        </div>

        <div id="webcamSection" class="hidden">
            <div class="flex justify-center space-x-4 mb-4">
                <button id="startWebcamButton" class="bg-green-600 text-white p-3 rounded-md font-semibold hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-offset-2 transition-colors">
                    Démarrer l'Analyse Webcam
                </button>
                <button id="stopWebcamButton" class="bg-red-600 text-white p-3 rounded-md font-semibold hover:bg-red-700 focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-offset-2 transition-colors" disabled>
                    Arrêter l'Analyse Webcam
                </button>
            </div>
            <div id="liveVideoContainer" class="video-container mt-4 mb-8 hidden">
                <div class="video-aspect-ratio">
                    <video id="webcamVideo" autoplay muted playsinline class="rounded-lg"></video>
                    <canvas id="webcamAnalysisCanvas" class="rounded-lg"></canvas>
                </div>
            </div>
            <div id="liveEmotionFeedback" class="hidden text-center text-gray-700 text-lg font-medium">
                Émotion détectée : <span id="currentEmotion" class="text-indigo-600">N/A</span>
            </div>
        </div>


        <div id="loadingMessage" class="hidden mt-6 flex items-center justify-center text-indigo-600 font-medium">
            <div class="spinner mr-3"></div>
            Analyse en cours... Cela peut prendre un certain temps.
        </div>

        <div id="errorMessage" class="hidden mt-6 p-4 bg-red-100 border border-red-400 text-red-700 rounded-md" role="alert">
            <strong class="font-bold">Erreur :</strong>
            <span id="errorText" class="block sm:inline"></span>
        </div>

        <div id="uploadedVideoDisplayContainer" class="hidden mt-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4 text-center">Vidéo Analysée (Enregistrée)</h2>
            <div class="video-container">
                <div class="video-aspect-ratio">
                    <video id="interviewVideo" controls class="rounded-lg"></video>
                    <canvas id="analysisCanvas" class="rounded-lg"></canvas>
                </div>
            </div>
        </div>

        <div id="resultsSummary" class="mt-8 pt-8 border-t border-gray-200 hidden">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">Résumé de l'Analyse</h2>
            <div id="transcriptionResult" class="mb-6 p-4 bg-gray-50 rounded-md">
                <h3 class="text-xl font-semibold text-gray-700 mb-2">1. Transcription de l'Entretien</h3>
                <p id="transcriptionText" class="text-gray-800 leading-relaxed"></p>
            </div>

            <div id="facialEmotionsResult" class="mb-6 p-4 bg-gray-50 rounded-md">
                <h3 class="text-xl font-semibold text-gray-700 mb-2">2. Analyse des Émotions Faciales</h3>
                <p id="facialEmotionsSummary" class="text-gray-800 leading-relaxed"></p>
            </div>

            <div id="sentimentResult" class="mb-6 p-4 bg-gray-50 rounded-md">
                <h3 class="text-xl font-semibold text-gray-700 mb-2">3. Analyse du Sentiment Général du Texte</h3>
                <p id="overallSentiment" class="text-gray-800 leading-relaxed"></p>
                <div id="rawSentimentScores" class="text-gray-700 text-sm mt-2"></div>
            </div>
        </div>
    </div>

    <script>
        const API_BASE_URL = 'http://127.0.0.1:8000'; 
        // For WebSocket, use 'ws://' for HTTP or 'wss://' for HTTPS
        const WS_BASE_URL = 'ws://127.0.0.1:8000'; 

        // --- Elements for Uploaded Video Analysis ---
        const uploadTab = document.getElementById('uploadTab');
        const webcamTab = document.getElementById('webcamTab');
        const uploadSection = document.getElementById('uploadSection');
        const webcamSection = document.getElementById('webcamSection');

        const uploadForm = document.getElementById('uploadForm');
        const videoFileInput = document.getElementById('videoFile');
        const analyzeButton = document.getElementById('analyzeButton');
        const uploadedVideoDisplayContainer = document.getElementById('uploadedVideoDisplayContainer');
        const interviewVideo = document.getElementById('interviewVideo');
        const analysisCanvas = document.getElementById('analysisCanvas');
        const ctx = analysisCanvas.getContext('2d');
        let currentAnalysisData = null; // Stores the full analysis data from the API
        let animationFrameId = null; // For requestAnimationFrame on uploaded video


        // --- Elements for Webcam Analysis ---
        const startWebcamButton = document.getElementById('startWebcamButton');
        const stopWebcamButton = document.getElementById('stopWebcamButton');
        const liveVideoContainer = document.getElementById('liveVideoContainer');
        const webcamVideo = document.getElementById('webcamVideo');
        const webcamAnalysisCanvas = document.getElementById('webcamAnalysisCanvas');
        const webcamCtx = webcamAnalysisCanvas.getContext('2d');
        const liveEmotionFeedback = document.getElementById('liveEmotionFeedback');
        const currentEmotionSpan = document.getElementById('currentEmotion');
        let mediaStream = null; // To store the webcam stream
        let webcamAnalysisInterval = null; // For setInterval on webcam frames
        let isAnalyzingWebcam = false;
        let ws = null; // WebSocket object for real-time analysis


        // --- Common Elements ---
        const loadingMessage = document.getElementById('loadingMessage');
        const errorMessage = document.getElementById('errorMessage');
        const errorText = document.getElementById('errorText');
        const resultsSummaryDiv = document.getElementById('resultsSummary');
        const transcriptionTextElem = document.getElementById('transcriptionText');
        const facialEmotionsSummaryElem = document.getElementById('facialEmotionsSummary');
        const overallSentimentElem = document.getElementById('overallSentiment');
        const rawSentimentScoresElem = document.getElementById('rawSentimentScores');


        // --- Tab Switching Logic ---
        function switchTab(activeTabId) {
            uploadTab.classList.remove('border-indigo-600', 'text-indigo-600');
            uploadTab.classList.add('border-transparent', 'text-gray-500', 'hover:text-gray-700');
            webcamTab.classList.remove('border-indigo-600', 'text-indigo-600');
            webcamTab.classList.add('border-transparent', 'text-gray-500', 'hover:text-gray-700');

            uploadSection.classList.add('hidden');
            webcamSection.classList.add('hidden');

            // Hide analysis results and video display when switching tabs
            hideAllAnalysisDisplays();

            if (activeTabId === 'uploadTab') {
                uploadTab.classList.add('border-indigo-600', 'text-indigo-600');
                uploadSection.classList.remove('hidden');
                // Ensure webcam is off if switching back
                stopWebcamAnalysis();
            } else if (activeTabId === 'webcamTab') {
                webcamTab.classList.add('border-indigo-600', 'text-indigo-600');
                webcamSection.classList.remove('hidden');
                // Prepare webcam section
                liveVideoContainer.classList.add('hidden');
                liveEmotionFeedback.classList.add('hidden');
                stopWebcamButton.disabled = true;
                startWebcamButton.disabled = false;
            }
        }

        uploadTab.addEventListener('click', () => switchTab('uploadTab'));
        webcamTab.addEventListener('click', () => switchTab('webcamTab'));

        function hideAllAnalysisDisplays() {
            loadingMessage.classList.add('hidden');
            errorMessage.classList.add('hidden');
            resultsSummaryDiv.classList.add('hidden');
            uploadedVideoDisplayContainer.classList.add('hidden');
            interviewVideo.src = '';
            ctx.clearRect(0, 0, analysisCanvas.width, analysisCanvas.height);
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
            }
            // For webcam specific displays
            liveVideoContainer.classList.add('hidden');
            liveEmotionFeedback.classList.add('hidden');
        }

        // --- Common Error/Loading/Display Functions ---
        function displayError(message) {
            errorMessage.classList.remove('hidden');
            errorText.textContent = message;
            hideAllAnalysisDisplays(); // Hide everything else on error
        }

        function hideError() {
            errorMessage.classList.add('hidden');
            errorText.textContent = '';
        }

        // --- Uploaded Video Analysis Logic ---
        function setupUploadedVideoAndCanvas(videoFileUrl) {
            uploadedVideoDisplayContainer.classList.remove('hidden');
            interviewVideo.src = videoFileUrl;

            interviewVideo.onloadedmetadata = () => {
                analysisCanvas.width = interviewVideo.videoWidth;
                analysisCanvas.height = interviewVideo.videoHeight;
                const aspectRatio = (interviewVideo.videoHeight / interviewVideo.videoWidth) * 100;
                document.querySelector('#uploadedVideoDisplayContainer .video-aspect-ratio').style.paddingBottom = `${aspectRatio}%`;
                interviewVideo.play();
            };

            interviewVideo.onplay = () => {
                drawUploadedVideoAnalysisLoop();
            };

            interviewVideo.onpause = () => {
                cancelAnimationFrame(animationFrameId);
            };
            interviewVideo.onended = () => {
                cancelAnimationFrame(animationFrameId);
                ctx.clearRect(0, 0, analysisCanvas.width, analysisCanvas.height);
            };
        }

        function drawUploadedVideoAnalysisLoop() {
            ctx.clearRect(0, 0, analysisCanvas.width, analysisCanvas.height);

            if (currentAnalysisData && interviewVideo.currentTime !== undefined) {
                const currentTime = Math.floor(interviewVideo.currentTime);
                const emotionsForCurrentSecond = currentAnalysisData.video_emotions.emotions_timeline.find(
                    item => item.timestamp_seconds === currentTime
                );

                if (emotionsForCurrentSecond && emotionsForCurrentSecond.detected_faces.length > 0) {
                    emotionsForCurrentSecond.detected_faces.forEach(face => {
                        const bbox = face.bounding_box;
                        const emotion = face.emotion;
                        const confidence = face.confidence;

                        const scaleX = analysisCanvas.width / interviewVideo.videoWidth;
                        const scaleY = analysisCanvas.height / interviewVideo.videoHeight;

                        const x = bbox.x * scaleX;
                        const y = bbox.y * scaleY;
                        const width = bbox.width * scaleX;
                        const height = bbox.height * scaleY;

                        ctx.strokeStyle = '#6366f1';
                        ctx.lineWidth = 4;
                        ctx.strokeRect(x, y, width, height);

                        ctx.fillStyle = '#6366f1';
                        ctx.font = `${Math.max(16, Math.floor(height / 8))}px Inter`;
                        ctx.fillText(`${emotion.charAt(0).toUpperCase() + emotion.slice(1)} (${(confidence * 100).toFixed(1)}%)`, x + 5, y - 10);
                    });
                }
            }
            animationFrameId = requestAnimationFrame(drawUploadedVideoAnalysisLoop);
        }

        uploadForm.addEventListener('submit', async (event) => {
            event.preventDefault();

            hideError();
            hideAllAnalysisDisplays(); // Ensure all displays are hidden
            loadingMessage.classList.remove('hidden');
            analyzeButton.disabled = true;

            const videoFile = videoFileInput.files[0];

            if (!videoFile) {
                displayError('Veuillez sélectionner un fichier vidéo.');
                loadingMessage.classList.add('hidden');
                analyzeButton.disabled = false;
                return;
            }

            const formData = new FormData();
            formData.append('video', videoFile);

            try {
                const response = await fetch(`${API_BASE_URL}/analyze-interview-video`, {
                    method: 'POST',
                    body: formData,
                });

                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(errorData.detail || `Erreur du serveur: ${response.status} ${response.statusText}`);
                }

                currentAnalysisData = await response.json(); // Store the full analysis data
                
                const videoFileUrl = URL.createObjectURL(videoFile);
                setupUploadedVideoAndCanvas(videoFileUrl);

                displayResultsSummary(currentAnalysisData);

            } catch (error) {
                console.error('Error during API call:', error);
                displayError(`Échec de l'analyse : ${error.message}`);
            } finally {
                loadingMessage.classList.add('hidden');
                analyzeButton.disabled = false;
            }
        });

        // --- Webcam Analysis Logic (UPDATED FOR WEBSOCKET) ---
        startWebcamButton.addEventListener('click', startWebcamAnalysis);
        stopWebcamButton.addEventListener('click', stopWebcamAnalysis);

        async function startWebcamAnalysis() {
            hideError();
            hideAllAnalysisDisplays(); 
            liveVideoContainer.classList.remove('hidden');
            liveEmotionFeedback.classList.remove('hidden');
            startWebcamButton.disabled = true;
            stopWebcamButton.disabled = false;
            isAnalyzingWebcam = true; // Set flag for ongoing analysis

            try {
                // Request both video and audio streams
                mediaStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true }); 
                webcamVideo.srcObject = mediaStream;
                webcamVideo.onloadedmetadata = () => {
                    webcamAnalysisCanvas.width = webcamVideo.videoWidth;
                    webcamAnalysisCanvas.height = webcamVideo.videoHeight;
                    const aspectRatio = (webcamVideo.videoHeight / webcamVideo.videoWidth) * 100;
                    document.querySelector('#liveVideoContainer .video-aspect-ratio').style.paddingBottom = `${aspectRatio}%`;
                    webcamVideo.play();
                    setupWebSocketAnalysis(); // Initiate WebSocket connection
                };
            } catch (err) {
                console.error("Erreur d'accès à la webcam : ", err);
                displayError("Impossible d'accéder à la webcam. Assurez-vous d'avoir autorisé l'accès et qu'aucune autre application ne l'utilise.");
                stopWebcamAnalysis(); // Reset buttons and state
            }
        }

        function stopWebcamAnalysis() {
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                webcamVideo.srcObject = null;
            }
            if (webcamAnalysisInterval) {
                clearInterval(webcamAnalysisInterval);
                webcamAnalysisInterval = null;
            }
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.close(); // Close WebSocket connection
            }
            webcamCtx.clearRect(0, 0, webcamAnalysisCanvas.width, webcamAnalysisCanvas.height); // Clear canvas
            liveVideoContainer.classList.add('hidden');
            liveEmotionFeedback.classList.add('hidden');
            startWebcamButton.disabled = false;
            stopWebcamButton.disabled = true;
            isAnalyzingWebcam = false;
            currentEmotionSpan.textContent = 'N/A';
        }

        function setupWebSocketAnalysis() {
            // Establish WebSocket connection
            ws = new WebSocket(`${WS_BASE_URL}/ws/analyze-webcam`); // This endpoint needs to be implemented in your FastAPI backend

            ws.onopen = () => {
                console.log("WebSocket connected for real-time analysis.");
                startSendingFramesOverWebSocket(); // Start sending frames once connected
            };

            ws.onmessage = (event) => {
                // Parse the JSON data sent from the backend
                try {
                    const analysisData = JSON.parse(event.data);
                    // console.log("Received real-time analysis:", analysisData); // For debugging

                    if (analysisData.facial_emotions) {
                        drawWebcamAnalysisOverlay(analysisData.facial_emotions);
                        updateLiveEmotionFeedback(analysisData.facial_emotions);
                    }
                    // Add more real-time feedback updates here for other metrics (e.g., speech rate, gaze)
                    // if (analysisData.speech_rate) { /* Update UI */ }
                    // if (analysisData.gaze_direction) { /* Update UI */ }

                } catch (e) {
                    console.error("Failed to parse WebSocket message:", e, event.data);
                }
            };

            ws.onerror = (error) => {
                console.error("WebSocket error:", error);
                displayError("Erreur de connexion WebSocket. L'analyse en temps réel est interrompue.");
                stopWebcamAnalysis(); // Stop analysis on error
            };

            ws.onclose = (event) => {
                console.log("WebSocket closed:", event);
                // Only show error if it wasn't intentionally stopped by the user
                if (isAnalyzingWebcam) { 
                    displayError("Connexion à l'analyse en temps réel perdue.");
                }
                stopWebcamAnalysis(); // Ensure all resources are cleaned up
            };
        }

        function startSendingFramesOverWebSocket() {
            const frameSendIntervalMs = 200; // Send frames every 200ms (5 frames/sec) for visual analysis

            // Clear previous interval if any
            if (webcamAnalysisInterval) {
                clearInterval(webcamAnalysisInterval);
            }

            webcamAnalysisInterval = setInterval(() => {
                // Ensure webcam is active and WebSocket is open
                if (!isAnalyzingWebcam || webcamVideo.paused || webcamVideo.ended || ws.readyState !== WebSocket.OPEN) {
                    return;
                }

                // Create a temporary canvas to draw the current webcam frame
                const tempCanvas = document.createElement('canvas');
                tempCanvas.width = webcamVideo.videoWidth;
                tempCanvas.height = webcamVideo.videoHeight;
                const tempCtx = tempCanvas.getContext('2d');
                
                // Draw the video frame onto the temporary canvas
                tempCtx.drawImage(webcamVideo, 0, 0, tempCanvas.width, tempCanvas.height);

                // Convert the canvas content to a JPEG Blob for sending
                tempCanvas.toBlob((blob) => {
                    if (blob) {
                        ws.send(blob); // Send the image blob over WebSocket
                    }
                }, 'image/jpeg', 0.8); // Specify image format (JPEG) and quality

                // TODO: For audio analysis, you would also need to capture audio chunks
                // from mediaStream and send them over the same (or a separate) WebSocket.
                // This typically involves using MediaRecorder API to record small audio segments.

            }, frameSendIntervalMs);
        }

        function drawWebcamAnalysisOverlay(detectedFaces) {
            // Clear canvas for each frame before drawing new overlays
            webcamCtx.clearRect(0, 0, webcamAnalysisCanvas.width, webcamAnalysisCanvas.height);

            // Redraw video frame onto the canvas to ensure the background is always the latest video frame
            // This is important because the video element might be scaled or hidden by the canvas.
            // Also, this allows you to apply transformations (like mirroring) to the canvas content.
            webcamCtx.drawImage(webcamVideo, 0, 0, webcamAnalysisCanvas.width, webcamAnalysisCanvas.height);

            // Undo the mirroring for drawing (so text and boxes appear correctly to the user)
            webcamCtx.save();
            webcamCtx.scale(-1, 1);
            webcamCtx.translate(-webcamAnalysisCanvas.width, 0);

            detectedFaces.forEach(face => {
                const bbox = face.bounding_box;
                const emotion = face.emotion;
                const confidence = face.confidence;

                // Bounding box coordinates are relative to the video frame, so they apply directly to canvas
                const x = bbox.x;
                const y = bbox.y;
                const width = bbox.width;
                const height = bbox.height;

                // Draw bounding box
                webcamCtx.strokeStyle = '#3b82f6'; // Tailwind blue-500
                webcamCtx.lineWidth = 4;
                webcamCtx.strokeRect(x, y, width, height);

                // Draw emotion label
                webcamCtx.fillStyle = '#3b82f6'; // Tailwind blue-500
                webcamCtx.font = `${Math.max(18, Math.floor(height / 6))}px Inter`; // Dynamic font size
                webcamCtx.fillText(`${emotion.charAt(0).toUpperCase() + emotion.slice(1)} (${(confidence * 100).toFixed(1)}%)`, x + 5, y - 10);
            });
            webcamCtx.restore(); // Restore context to original state (un-mirror)
        }

        function updateLiveEmotionFeedback(detectedFaces) {
            if (detectedFaces.length > 0) {
                // Find the emotion with the highest confidence among all detected faces in the current frame
                let topEmotion = detectedFaces[0].emotion;
                let topConfidence = detectedFaces[0].confidence;

                for (let i = 1; i < detectedFaces.length; i++) {
                    if (detectedFaces[i].confidence > topConfidence) {
                        topEmotion = detectedFaces[i].emotion;
                        topConfidence = detectedFaces[i].confidence;
                    }
                }
                currentEmotionSpan.textContent = `${topEmotion.charAt(0).toUpperCase() + topEmotion.slice(1)} (${(topConfidence * 100).toFixed(1)}%)`;
            } else {
                currentEmotionSpan.textContent = 'Aucun visage détecté';
            }
        }


        // --- Display Text Summary Results for Uploaded Videos ---
        function displayResultsSummary(data) {
            resultsSummaryDiv.classList.remove('hidden');

            // 1. Transcription
            transcriptionTextElem.textContent = data.transcription.text;

            // 2. Facial Emotions Summary (for uploaded video)
            const videoDuration = data.video_emotions.video_duration_seconds;
            const emotionsTimeline = data.video_emotions.emotions_timeline;

            let facialSummary = `Sur une durée de **${videoDuration} secondes**, en analysant ${emotionsTimeline.length} images clés (une par seconde) : `;

            let emotionCounts = {};
            emotionsTimeline.forEach(frameData => {
                frameData.detected_faces.forEach(faceData => {
                    const emotion = faceData.emotion;
                    emotionCounts[emotion] = (emotionCounts[emotion] || 0) + 1;
                });
            });

            let dominantEmotion = null;
            let maxCount = 0;
            if (Object.keys(emotionCounts).length > 0) {
                for (const emotion in emotionCounts) {
                    if (emotionCounts[emotion] > maxCount) {
                        maxCount = emotionCounts[emotion];
                        dominantEmotion = emotion;
                    }
                }
                facialSummary += `L'émotion faciale la plus fréquemment détectée était la **${dominantEmotion.charAt(0).toUpperCase() + dominantEmotion.slice(1)}**.\n`;
            } else {
                facialSummary += `Aucun visage significatif n'a été détecté ou aucune émotion dominante claire.\n`;
            }

            let keyEmotionTimestamps = {};
            emotionsTimeline.forEach(frameData => {
                frameData.detected_faces.forEach(faceData => {
                    const emotion = faceData.emotion;
                    if (dominantEmotion && emotion !== dominantEmotion) {
                        if (!keyEmotionTimestamps[emotion]) {
                            keyEmotionTimestamps[emotion] = new Set();
                        }
                        keyEmotionTimestamps[emotion].add(frameData.timestamp_seconds);
                    }
                });
            });

            if (Object.keys(keyEmotionTimestamps).length > 0) {
                facialSummary += "Cependant, il est important de noter des moments où d'autres émotions ont été observées :\n";
                for (const emo in keyEmotionTimestamps) {
                    const sortedTimestamps = Array.from(keyEmotionTimestamps[emo]).sort((a,b) => a-b).join(', ');
                    facialSummary += `* Des expressions de **${emo.charAt(0).toUpperCase() + emo.slice(1)}** ont été détectées autour des secondes : **${sortedTimestamps}**.\n`;
                }
                facialSummary += "\nCes variations émotionnelles sont intéressantes à considérer en regard du discours.\n";
            } else if (dominantEmotion) {
                facialSummary += `\nL'émotion ${dominantEmotion.charAt(0).toUpperCase() + dominantEmotion.slice(1)} a été prédominante tout au long des moments analysés.\n`;
            }
            facialEmotionsSummaryElem.innerHTML = facialSummary.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');


            // 3. Overall Sentiment
            const overallSentiment = data.overall_text_sentiment.overall_sentiment;
            const confidenceScore = data.overall_text_sentiment.confidence_score;
            const rawScores = data.overall_text_sentiment.raw_scores;

            overallSentimentElem.innerHTML = `Sur la base de la transcription de votre réponse, le sentiment général détecté est <strong>${overallSentiment.charAt(0).toUpperCase() + overallSentiment.slice(1)}</strong>, avec un score de confiance de <strong>${(confidenceScore * 100).toFixed(1)}%</strong>.`;

            rawSentimentScoresElem.innerHTML = '<p class="font-semibold mb-1">Détails des scores de sentiment :</p>';
            let scoresList = '<ul>';
            for (const label in rawScores) {
                scoresList += `<li>${label.charAt(0).toUpperCase() + label.slice(1)}: ${(rawScores[label] * 100).toFixed(1)}%</li>`;
            }
            scoresList += '</ul>';
            rawSentimentScoresElem.innerHTML += scoresList;
        }

        // Initialize with Upload tab active
        switchTab('uploadTab');
    </script>
</body>
</html>